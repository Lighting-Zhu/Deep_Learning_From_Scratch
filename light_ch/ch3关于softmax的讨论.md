### 实现softmax函数时的注意事项

softmax function
$$
y_k=\dfrac{e^{a_k}}{\sum_{i=1}^ne^{a_k}}
$$
  假设输出层共有n个神经元，计算第$k$个神经元的输出$y_k$。  

​    ![img](ch3%E5%85%B3%E4%BA%8Esoftmax%E7%9A%84%E8%AE%A8%E8%AE%BA.assets/v2-ece5a177f0001f1b858369b4675e67ef_hd.png)

将输入[3,1,-3]通过softmax函数作用，映射成为(0,1)的值为(0.88,0.12,0), 而这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标！ 

### 存在问题

当我们碰到运算值比较大的时候，会出现数值溢出问题。

e.g. $[1000,1000,1000]$ 以及 $[-1000,-999,-1000]$

这是因为浮点数只有64位，在计算指数函数的环节，$exp(1000)=inf$，会发生上溢出；$exp(-1000)=0$，会发生下溢出；

### 解决办法

$$
log\sum_{n=1}^Ne^{x_n}=a+log\sum_{n=1}^Ne^{x_n-a}
$$

上式对于任意$a$都成立，意味着可以自由调节指数函数的指数部分。一个典型的做法是取${x_1,x_2,...，x_n}$

这样可以保证指数最大不会超过0。因此也不会出现上溢出与下溢出。

### softmax的常数位移证明

即$softmax(x)=softmax(x+c)$
$$
(softmax(c+x))_i=\dfrac{e^{x_i+c}}{\sum_je^{x_j+c}}=\dfrac{e^x_i\times e^c}{e^c\times \sum_je^{x_j}}=(softmax(x))i
$$
